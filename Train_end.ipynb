{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we experiment with a binary classification for the ending sentence for stories using our text classification model we developed in HW4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available: True\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Cuda available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********TEST SET**********\n",
      "\n",
      "TRUE ENDING:\t The next weekend, I was asked to please stay home.\n",
      "FALSE ENDING:\t My friends decided to keep inviting me out as I am so much fun.\n",
      "\n",
      "\n",
      "TRUE ENDING:\t My allergies were too bad and I had to go back home.\n",
      "FALSE ENDING:\t It reminded me of how much I loved spring flowers.\n",
      "\n",
      "\n",
      "TRUE ENDING:\t Avery regretted what she did the next day.\n",
      "FALSE ENDING:\t Avery thought her children would be happy with her decision.\n",
      "\n",
      "\n",
      "TRUE ENDING:\t Josh got sick.\n",
      "FALSE ENDING:\t Josh thought that the pie was delicious.\n",
      "\n",
      "\n",
      "TRUE ENDING:\t He felt inspiration and then went back home to write.\n",
      "FALSE ENDING:\t John then got an idea for his painting.\n",
      "\n",
      "\n",
      "TRUE ENDING:\t The cashier handed them the cash so they would go away.\n",
      "FALSE ENDING:\t The cashier invited the men to her high school reunion.\n",
      "\n",
      "\n",
      "TRUE ENDING:\t They called the police to come to my house.\n",
      "FALSE ENDING:\t They liked me a lot after that.\n",
      "\n",
      "\n",
      "TRUE ENDING:\t Kerry was so grateful!\n",
      "FALSE ENDING:\t Kerry was disappointed.\n",
      "\n",
      "\n",
      "TRUE ENDING:\t Franny learned to examine her prejudices.\n",
      "FALSE ENDING:\t Franny ended up getting deported.\n",
      "\n",
      "\n",
      "TRUE ENDING:\t Luckily he had been saving everything on an external disk.\n",
      "FALSE ENDING:\t When he finished his paper he went to bed.\n",
      "\n",
      "\n",
      "TRUE ENDING:\t Ari opens the jar to find perfect pickles.\n",
      "FALSE ENDING:\t Ari's pickles are sweet.\n",
      "\n",
      "\n",
      "TRUE ENDING:\t Trudey hoped self-publishing would be more profitable.\n",
      "FALSE ENDING:\t Trudey called her sister and asked her to come to dinner.\n",
      "\n",
      "\n",
      "TRUE ENDING:\t Feliciano was happy about his nice day.\n",
      "FALSE ENDING:\t The pair then went out to pick olives.\n",
      "\n",
      "\n",
      "TRUE ENDING:\t Aaron suggested he help her cook another meal instead.\n",
      "FALSE ENDING:\t Aaron broke up with her.\n",
      "\n",
      "\n",
      "TRUE ENDING:\t Oliver was so grateful for his wife's love.\n",
      "FALSE ENDING:\t Oliver decided to not get married.\n",
      "\n",
      "\n",
      "TRUE ENDING:\t Somebody found her cat.\n",
      "FALSE ENDING:\t Sarah broke her leg.\n",
      "\n",
      "\n",
      "TRUE ENDING:\t Kelly was so happy to finally beat it.\n",
      "FALSE ENDING:\t Kelly was mad about that.\n",
      "\n",
      "\n",
      "TRUE ENDING:\t Deb said yes to Tim's marriage proposal.\n",
      "FALSE ENDING:\t Deb told Tim she was only interested in woman.\n",
      "\n",
      "\n",
      "TRUE ENDING:\t I loved them anyway.\n",
      "FALSE ENDING:\t I thought the cereal was delicious.\n",
      "\n",
      "\n",
      "TRUE ENDING:\t I have very fond memories of checkers.\n",
      "FALSE ENDING:\t To this day I hate checkers.\n",
      "\n",
      "\n",
      "**********VALIDATION SET**********\n",
      "\n",
      "TRUE ENDING:\t He is happy now.\n",
      "FALSE ENDING:\t He joined a gang.\n",
      "\n",
      "\n",
      "TRUE ENDING:\t The brownies are so delicious Laverne eats two of them.\n",
      "FALSE ENDING:\t Laverne doesn't go to her friend's party.\n",
      "\n",
      "\n",
      "TRUE ENDING:\t Sarah decided that she preferred her home over Europe.\n",
      "FALSE ENDING:\t Sarah then decided to move to Europe.\n",
      "\n",
      "\n",
      "TRUE ENDING:\t Gina liked the cookies so much she ate them all in one sitting.\n",
      "FALSE ENDING:\t Gina gave the cookies away at her church.\n",
      "\n",
      "\n",
      "TRUE ENDING:\t I was very proud of my performance.\n",
      "FALSE ENDING:\t I was very ashamed of my performance.\n",
      "\n",
      "\n",
      "TRUE ENDING:\t I never gave the man money again.\n",
      "FALSE ENDING:\t The next day I gave the man twenty dollars.\n",
      "\n",
      "\n",
      "TRUE ENDING:\t Jim took time to decide what he would take a picture of.\n",
      "FALSE ENDING:\t Jim took 20 more photos.\n",
      "\n",
      "\n",
      "TRUE ENDING:\t His boss commends him for a job well done.\n",
      "FALSE ENDING:\t Ron is immediately fired for insubordination.\n",
      "\n",
      "\n",
      "TRUE ENDING:\t In Vegas, John and Billy competed against eighty contestants.\n",
      "FALSE ENDING:\t John and Billy were disappointed.\n",
      "\n",
      "\n",
      "TRUE ENDING:\t Later, she passed the test.\n",
      "FALSE ENDING:\t But she gave up.\n",
      "\n",
      "\n",
      "TRUE ENDING:\t Eventually Trish grudgingly came to accept nature.\n",
      "FALSE ENDING:\t The fish had very interesting mating habits.\n",
      "\n",
      "\n",
      "TRUE ENDING:\t When Corey came to, both the man and his car were gone.\n",
      "FALSE ENDING:\t Corey didn't mind at all.\n",
      "\n",
      "\n",
      "TRUE ENDING:\t Marcy gave her boyfriend a big kiss and said yes.\n",
      "FALSE ENDING:\t Marcy shrugged her shoulders and reached for the TV remote.\n",
      "\n",
      "\n",
      "TRUE ENDING:\t Cara decided to eat at the food cart every week.\n",
      "FALSE ENDING:\t Cara told the owner that the food was awful.\n",
      "\n",
      "\n",
      "TRUE ENDING:\t Tony enjoyed going on the cruise.\n",
      "FALSE ENDING:\t Tony was scared of the ocean.\n",
      "\n",
      "\n",
      "TRUE ENDING:\t Ignacio won a silver medal.\n",
      "FALSE ENDING:\t Ignacio gave up swimming.\n",
      "\n",
      "\n",
      "TRUE ENDING:\t They prepared for the start of the race.\n",
      "FALSE ENDING:\t Danny decided to go to sleep.\n",
      "\n",
      "\n",
      "TRUE ENDING:\t Mary couldn't wait to present her report.\n",
      "FALSE ENDING:\t Mary disliked Pandas.\n",
      "\n",
      "\n",
      "TRUE ENDING:\t Ellen ended up winning the prize.\n",
      "FALSE ENDING:\t Ellen had to pay a replacement fee for her library card.\n",
      "\n",
      "\n",
      "TRUE ENDING:\t Jesse was picked on by the other boys in school.\n",
      "FALSE ENDING:\t Jesse went to the park with his new friend, Roger.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the datasets\n",
    "data_train_raw = pd.read_csv(\"ROCStories_winter2017.csv\")\n",
    "data_test_raw = pd.read_csv(\"cloze_test_2016.csv\")\n",
    "data_val_raw = pd.read_csv(\"cloze_val_2016.csv\")\n",
    "\n",
    "\n",
    "#test set \n",
    "endings = data_test_raw.RandomFifthSentenceQuiz1.tolist(),data_test_raw.RandomFifthSentenceQuiz2.tolist()\n",
    "answers =data_test_raw.AnswerRightEnding.tolist()\n",
    "#seperate the endings for each example into appropriate lists\n",
    "test_true = []\n",
    "test_false = []\n",
    "num_instances , num_columns = data_test_raw.shape\n",
    "for i in range(num_instances):\n",
    "    test_true.append(endings[answers[i]-1][i])\n",
    "    test_false.append(endings[(answers[i]+2)%2][i])\n",
    "    \n",
    "\n",
    "# validation set (duplicate code from above, probably couldve just made a function to process the test and\n",
    "# val sets but oh well)\n",
    "val_endings = data_val_raw.RandomFifthSentenceQuiz1.tolist(),data_val_raw.RandomFifthSentenceQuiz2.tolist()\n",
    "val_answers =data_val_raw.AnswerRightEnding.tolist()\n",
    "#seperate the endings for each example into appropriate lists\n",
    "val_true = []\n",
    "val_false = []\n",
    "num_instances , num_columns = data_val_raw.shape\n",
    "for i in range(num_instances):\n",
    "    val_true.append(val_endings[val_answers[i]-1][i])\n",
    "    val_false.append(val_endings[(val_answers[i]+2)%2][i])\n",
    "    \n",
    "#check the first 20 examples in the test and val set to see how it looks\n",
    "print(\"**********TEST SET**********\\n\")\n",
    "for i in range(20):\n",
    "    print(\"TRUE ENDING:\\t\", test_true[i])\n",
    "    print(\"FALSE ENDING:\\t\", test_false[i])\n",
    "    print(\"\\n\")\n",
    "    \n",
    "print(\"**********VALIDATION SET**********\\n\")\n",
    "for i in range(20):\n",
    "    print(\"TRUE ENDING:\\t\", val_true[i])\n",
    "    print(\"FALSE ENDING:\\t\", val_false[i])\n",
    "    print(\"\\n\")\n",
    "    \n",
    "#lets create a list of tuples for each of these sets\n",
    "test_set = []\n",
    "val_set = []\n",
    "#test set\n",
    "for i in range(len(test_true)):\n",
    "    test_set.append((test_true[i], 'Coherent'))\n",
    "for i in range(len(test_false)):\n",
    "    test_set.append((test_false[i], 'Incoherent'))\n",
    "#val set\n",
    "for i in range(len(val_true)):\n",
    "    val_set.append((val_true[i], 'Coherent'))\n",
    "for i in range(len(val_false)):\n",
    "    val_set.append((val_false[i], 'Incoherent'))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vocabulary class courtesy of HW4\n",
    "\n",
    "class MyVocabulary:\n",
    "    def __init__(self, special_tokens=None):\n",
    "        self.w2idx = {}\n",
    "        self.idx2w = {}\n",
    "        self.w2cnt = defaultdict(int)\n",
    "        self.special_tokens = special_tokens\n",
    "        if self.special_tokens is not None:\n",
    "            self.add_tokens(special_tokens)\n",
    "\n",
    "    def add_tokens(self, tokens):\n",
    "        for token in tokens:\n",
    "            self.add_token(token)\n",
    "            self.w2cnt[token] += 1\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token not in self.w2idx:\n",
    "            cur_len = len(self)\n",
    "            self.w2idx[token] = cur_len\n",
    "            self.idx2w[cur_len] = token\n",
    "\n",
    "    def prune(self, min_cnt=2):\n",
    "        to_remove = set([token for token in self.w2idx if self.w2cnt[token] < min_cnt])\n",
    "        #print(set(self.special_tokens))\n",
    "        \n",
    "        #INSERTED THESE CHANGES MYSELF\n",
    "        for token in self.special_tokens:\n",
    "            if token in to_remove:\n",
    "                to_remove.remove(token)\n",
    "        #THE ORIGINAL CODE IN PRUNE WAS REMOVING <PAD> FROM THE VOCAB\n",
    "        #to_remove ^= set(self.special_tokens)\n",
    "        #print('<PAD>' in to_remove,self.w2cnt['<PAD>'])\n",
    "\n",
    "        for token in to_remove:\n",
    "            self.w2cnt.pop(token)\n",
    "\n",
    "        self.w2idx = {token: idx for idx, token in enumerate(self.w2cnt.keys())}\n",
    "        self.idx2w = {idx: token for token, idx in self.w2idx.items()}\n",
    "        \n",
    "        #print(\"Pad in w2cnt\", '<PAD>' in self.w2cnt)\n",
    "        #print(\"Pad in w2idx\", '<PAD>' in self.w2idx)\n",
    "        #print(\"Pad in idx2w\", self.idx2w[0] is '<PAD>')\n",
    "        #print(self.w2idx['<PAD>'])\n",
    "\n",
    "    def __contains__(self, item):\n",
    "        return item in self.w2idx\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if isinstance(item, str):\n",
    "            return self.w2idx[item]\n",
    "        elif isinstance(item , int):\n",
    "            return self.idx2w[item]\n",
    "        else:\n",
    "            raise TypeError(\"Supported indices are int and str\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return(len(self.w2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text classification dataset from HW4\n",
    "#manually set max length of sentence to 13. arbitrarily chosen number\n",
    "\n",
    "class TextClassificationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels, vocab=None, labels_vocab=None, max_len=13, lowercase=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texts (list of str): texts of the dataset examples\n",
    "            labels (list of str): the correponding labels of the dataset examples\n",
    "            vocab (MyVocabulary, optional): vocabular to convert text to indices. If not provided, will be created based on the texts\n",
    "            labels_vocab (MyVocabulary, optional): vocabular to convert labels to indices. If not provided, will be created based on the labels\n",
    "            max_len (int): maximum length of the text. Texts shorter than max_len will be cut at the end\n",
    "            lowercase (bool, optional): a fag specifying whether or not the input text should be lowercased\n",
    "        \"\"\"\n",
    "        \n",
    "        self.max_len = max_len\n",
    "        self.lowercase = lowercase\n",
    "        \n",
    "\n",
    "        self.texts = [self._preprocess(t, max_len=max_len, lowercase=lowercase) for t in texts]\n",
    "        self.labels = labels\n",
    "\n",
    "        if vocab is None:\n",
    "            vocab = MyVocabulary(['<PAD>', '<UNK>'])\n",
    "            for text in self.texts:\n",
    "                vocab.add_tokens(text)\n",
    "            \n",
    "        if labels_vocab is None:\n",
    "            labels_vocab = MyVocabulary()\n",
    "            labels_vocab.add_tokens(labels)\n",
    "            \n",
    "        self.vocab = vocab\n",
    "        self.labels_vocab = labels_vocab\n",
    "        \n",
    "    def _preprocess(self, text, max_len=None, lowercase=True):\n",
    "        \"\"\"\n",
    "        Preprocess a give dataset example\n",
    "        Args:\n",
    "            text (str): given dataset example\n",
    "            max_len (int, optional): maximum sequence length\n",
    "            lowercase (bool, optional): a fag specifying whether or not the input text should be lowercased\n",
    "        \n",
    "        Returns:\n",
    "            a list of tokens for a given text span\n",
    "        \"\"\" \n",
    "        ### YOUR CODE BELOW ###\n",
    "        if lowercase:\n",
    "            text = text.lower()\n",
    "        # tokenize the input text\n",
    "       \n",
    "        tokens = []\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # cut the list of tokens to `max_len` if needed \n",
    "        if len(tokens) > max_len:\n",
    "            del tokens[max_len:]\n",
    "        tokens = self._pad(tokens)\n",
    "        ### YOUR CODE ABOVE ###\n",
    "        return tokens\n",
    "    \n",
    "    def _pad(self, tokens):\n",
    "        \"\"\"\n",
    "        Pad tokens to self.max_len\n",
    "        Args:\n",
    "            tokens (list): a list of str tokens for a given example\n",
    "            \n",
    "        Returns:\n",
    "            list: a padded list of str tokens for a given example\n",
    "        \"\"\"\n",
    "        # pad the list of tokens to be exactly of the `max_len` size\n",
    "        ### YOUR CODE BELOW ###\n",
    "        if len(tokens) < self.max_len:\n",
    "            for i in range(len(tokens), self.max_len):\n",
    "                tokens.append(\"<PAD>\") \n",
    "        ### YOUR CODE ABOVE ###\n",
    "        return tokens\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Given an index, return a formatted dataset example\n",
    "        \n",
    "        Args:\n",
    "            idx (int): dataset index\n",
    "            \n",
    "        Returns:\n",
    "            tuple: a tuple of token_ids based on the vocabulary mapping  and a corresponding label\n",
    "        \"\"\"\n",
    "        ### YOUR CODE BELOW ###\n",
    "        tokens = []\n",
    "        words = self.texts[idx]\n",
    "        for word in words:\n",
    "            if word not in self.vocab.w2cnt:\n",
    "                #print(word)\n",
    "                tokens.append(self.vocab.w2idx['<UNK>'])\n",
    "            elif word != '<PAD>':\n",
    "                tokens.append(self.vocab.w2idx[word])\n",
    "            elif word == '<PAD>':\n",
    "                tokens.append(self.vocab.w2idx[\"<PAD>\"])\n",
    "                #print(\"found pad\")\n",
    "        label = self.labels[idx]\n",
    "        ### YOUR CODE ABOVE ###\n",
    "        \n",
    "        return  np.asarray(tokens), np.asarray(self.labels_vocab.w2idx[label])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tommy was then treated and felt much better. Coherent\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.shuffle(test_set)\n",
    "random.shuffle(val_set)\n",
    "\n",
    "\n",
    "data_train = []\n",
    "labels_train = []\n",
    "\n",
    "data_test, labels_test = zip(*test_set)\n",
    "data_train, labels_train = zip(*val_set)\n",
    "\n",
    "print(data_train[0],labels_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = TextClassificationDataset(data_train, labels_train)\n",
    "dataset_test = TextClassificationDataset(data_test, labels_test, vocab=dataset_train.vocab, labels_vocab=dataset_train.labels_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3880 words before pruning\n",
      "2110 words after pruning\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_train.vocab.w2cnt), \"words before pruning\")\n",
    "dataset_train.vocab.prune()\n",
    "print(len(dataset_train.vocab.w2cnt),\"words after pruning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModel(torch.nn.Module):\n",
    "    def __init__(self, embedding_size, vocab_size, hidden_size, nb_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.nb_classes = nb_classes\n",
    "\n",
    "        ### YOUR CODE BELOW ###\n",
    "        self.embeddings = torch.nn.Embedding(vocab_size, embedding_size)\n",
    "        self.GRU = torch.nn.GRU(embedding_size, hidden_size, batch_first = True)\n",
    "        self.projection = torch.nn.Linear(hidden_size, nb_classes)\n",
    "        ### YOUR CODE ABOVE ###\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        ### YOUR CODE BELOW ###\n",
    "        embeds = self.embeddings(inputs)\n",
    "        outputs, garbage = self.GRU(embeds)\n",
    "        outputs = torch.max(outputs, dim = 1)[0]\n",
    "        #print(outputs)\n",
    "        logits = self.projection(outputs)\n",
    "        ### YOUR CODE ABOVE ###        \n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATALOADER #\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, shuffle=True, batch_size=64)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, shuffle=False, batch_size=64)\n",
    "\n",
    "# MODEL INITIALIZATION #\n",
    "model = TextClassificationModel(64, len(dataset_train.vocab), 128, 2)#2 labels\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "# OPTIMIZER #\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# LOSS-FUNCTION #\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss 0.6883983925237493\n",
      "Epoch 1, loss 0.6601119203082586\n",
      "Epoch 2, loss 0.6279697872824588\n",
      "Epoch 3, loss 0.5893391138416225\n",
      "Epoch 4, loss 0.5480969967478413\n",
      "Epoch 5, loss 0.49717101606272035\n",
      "Epoch 6, loss 0.4429547998864772\n",
      "Epoch 7, loss 0.39101992976867544\n",
      "Epoch 8, loss 0.32858305588617165\n",
      "Epoch 9, loss 0.26871523134789227\n"
     ]
    }
   ],
   "source": [
    "# TRAINING #\n",
    "losses = []\n",
    "for epoch in range(10):\n",
    "    epoch_losses = []\n",
    "    for i, batch in enumerate(dataloader_train):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x, y = batch\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "        \n",
    "        #print(y)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        \n",
    "        epoch_losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = np.mean(epoch_losses)\n",
    "    losses.append(epoch_loss)\n",
    "    print('Epoch {}, loss {}'.format(epoch, epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataloader):\n",
    "    \"\"\"\n",
    "    Predict probability distributions over classes on the test data\n",
    "    \n",
    "    Args:\n",
    "        model: your torch.nn.Module() model object\n",
    "        dataloader: test Dataloder() object\n",
    "        \n",
    "    Returns:\n",
    "        tuple: np.array/list with true labels and np.array/list with predicted labels\n",
    "    \"\"\"\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    \n",
    "    ### YOUR CODE BELOW ###\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        x, y = batch\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "            \n",
    "        outputs = model(x).detach().cpu()\n",
    "        y_pred.append(torch.argmax(outputs.data, 1))\n",
    "        y_true.append(y)\n",
    "        \n",
    "    ### YOUR CODE ABOVE ###\n",
    "        \n",
    "    y_pred = np.concatenate(y_pred, axis=0)\n",
    "    y_true = np.concatenate(y_true, axis=0)    \n",
    "    \n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_train, y_pred_train = predict(model, dataloader_train)\n",
    "y_true_test, y_pred_test = predict(model, dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train 0.946018172100481\n",
      "Accuracy test 0.61838588989845\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE BELOW ###\n",
    "acc_train = accuracy_score(y_true_train, y_pred_train)\n",
    "acc_test = accuracy_score(y_true_test, y_pred_test)\n",
    "### YOUR CODE ABOVE ###\n",
    "\n",
    "print('Accuracy train', acc_train)\n",
    "print('Accuracy test', acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
