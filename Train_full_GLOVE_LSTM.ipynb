{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we experiment with a binary classification for full five sentence stories using our text classification model we developed in HW4, with GLOVE embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available: True\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Cuda available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********TEST SET**********\n",
      "\n",
      "My friends all love to go to the club to dance. They think it's a lot of fun and always invite. I finally decided to tag along last Saturday. I danced terribly and broke a friend's toe.\n",
      "TRUE ENDING:\t The next weekend, I was asked to please stay home.\n",
      "FALSE ENDING:\t My friends decided to keep inviting me out as I am so much fun.\n",
      "\n",
      "\n",
      "I tried going to the park the other day. The weather seemed nice enough for a walk. Within minutes of getting there I started sneezing. My eyes were watery and it was hard to breathe.\n",
      "TRUE ENDING:\t My allergies were too bad and I had to go back home.\n",
      "FALSE ENDING:\t It reminded me of how much I loved spring flowers.\n",
      "\n",
      "\n",
      "Avery was married with children. She was tired of her boring life. One day, she decided to meet up with an old boyfriend from college. She made poor decisions that night and was unfaithful to her husband.\n",
      "TRUE ENDING:\t Avery regretted what she did the next day.\n",
      "FALSE ENDING:\t Avery thought her children would be happy with her decision.\n",
      "\n",
      "\n",
      "Josh loved when his mom baked apple pie. He hated how he always had to wait until after dinner though. So he decided this time he would sneak a piece before dinner. The eggs his mom used must have been bad though.\n",
      "TRUE ENDING:\t Josh got sick.\n",
      "FALSE ENDING:\t Josh thought that the pie was delicious.\n",
      "\n",
      "\n",
      "John was writing lyrics for his new album. He started experiencing writer's block. He tried to force himself to write but it wouldn't do anything. He took a walk, hung out with some friends, and looked at nature.\n",
      "TRUE ENDING:\t He felt inspiration and then went back home to write.\n",
      "FALSE ENDING:\t John then got an idea for his painting.\n",
      "\n",
      "\n",
      "The cashier was counting the dollar bills at her desk. Two men rushed into the store and held their guns up. Everyone panicked and started to scream. The men threatened the people to remain quiet.\n",
      "TRUE ENDING:\t The cashier handed them the cash so they would go away.\n",
      "FALSE ENDING:\t The cashier invited the men to her high school reunion.\n",
      "\n",
      "\n",
      "The Mills next door had a new car. The car was stolen during the weekend. They came to my house and asked me if I knew anything. I told them I didn't, but for some reason they suspected me.\n",
      "TRUE ENDING:\t They called the police to come to my house.\n",
      "FALSE ENDING:\t They liked me a lot after that.\n",
      "\n",
      "\n",
      "There was an extremely obnoxious boy in Kerry's economics class. He monopolized every class discussion with his mediocre ideas. The professor could never get him to be quiet. One day, he showed up to class with laryngitis.\n",
      "TRUE ENDING:\t Kerry was so grateful!\n",
      "FALSE ENDING:\t Kerry was disappointed.\n",
      "\n",
      "\n",
      "Franny did not particularly like all of the immigration happening. She thought immigrants were coming to cause social problems. Franny was upset when an immigrant moved in next door. The immigrant, Sal, was kind and became friends with Franny.\n",
      "TRUE ENDING:\t Franny learned to examine her prejudices.\n",
      "FALSE ENDING:\t Franny ended up getting deported.\n",
      "\n",
      "\n",
      "Brandon was given a 10 page research paper to write for class. He begun working on the paper on his 2003 Dell laptop. Brandon stayed up until 1 in the morning to try and finish his paper. Suddenly, his computer crashed and died.\n",
      "TRUE ENDING:\t Luckily he had been saving everything on an external disk.\n",
      "FALSE ENDING:\t When he finished his paper he went to bed.\n",
      "\n",
      "\n",
      "Ari spends $20 a day on pickles. He decides to make his own to save money. He puts the pickles in brine. Ari waits 2 weeks for his pickles to get sour.\n",
      "TRUE ENDING:\t Ari opens the jar to find perfect pickles.\n",
      "FALSE ENDING:\t Ari's pickles are sweet.\n",
      "\n",
      "\n",
      "Trudey wanted to write novels for a living. She wrote one through traditional publishing means. It barely made enough to cover the advance she had received. She wrote another through self-publishing avenues.\n",
      "TRUE ENDING:\t Trudey hoped self-publishing would be more profitable.\n",
      "FALSE ENDING:\t Trudey called her sister and asked her to come to dinner.\n",
      "\n",
      "\n",
      "Feliciano went olive picking with his grandmother. While they picked, she told him stories of his ancestors. Before he realized it, the sun was going down. They took the olives home and ate them together.\n",
      "TRUE ENDING:\t Feliciano was happy about his nice day.\n",
      "FALSE ENDING:\t The pair then went out to pick olives.\n",
      "\n",
      "\n",
      "Aaron's girlfriend asked him to come over for dinner. She said she was making his favorite, chicken alfredo. Aaron was very excited she wanted to cook, but he hated alfredo. She must have mixed up his words when he told her his least favorite.\n",
      "TRUE ENDING:\t Aaron suggested he help her cook another meal instead.\n",
      "FALSE ENDING:\t Aaron broke up with her.\n",
      "\n",
      "\n",
      "Oliver was nervous about his wedding. He was worried that he would stutter during the vows. When the time came, he took a deep breath and began to speak. He stuttered, but his wife smiled and hugged him and he was okay.\n",
      "TRUE ENDING:\t Oliver was so grateful for his wife's love.\n",
      "FALSE ENDING:\t Oliver decided to not get married.\n",
      "\n",
      "\n",
      "Sara had lost her cat. She was so sad! She put up signs all over the neighborhood. Then a wonderful thing happened.\n",
      "TRUE ENDING:\t Somebody found her cat.\n",
      "FALSE ENDING:\t Sarah broke her leg.\n",
      "\n",
      "\n",
      "Kelly was playing her new Mario game. She had been playing it for weeks. She was playing for so long without beating the level. Finally she beat the last level.\n",
      "TRUE ENDING:\t Kelly was so happy to finally beat it.\n",
      "FALSE ENDING:\t Kelly was mad about that.\n",
      "\n",
      "\n",
      "Tim and Deb have been dating for almost a year. They met at a party they attended last Christmas. Tim picked Deb up for a date tonight and took her to a fancy dinner. Just after dessert he proposed marriage.\n",
      "TRUE ENDING:\t Deb said yes to Tim's marriage proposal.\n",
      "FALSE ENDING:\t Deb told Tim she was only interested in woman.\n",
      "\n",
      "\n",
      "My roommate is from Germany. He is a very good cook. One day, he made some sausage for me that he ate back home in Germany. The sausage turned out poorly, according to him.\n",
      "TRUE ENDING:\t I loved them anyway.\n",
      "FALSE ENDING:\t I thought the cereal was delicious.\n",
      "\n",
      "\n",
      "When I was a kid I really wanted to play checkers. I sat down with my grandpa and he taught me. At first, he was just teaching me but it became a special thing. As I got older, I continued to play checkers with him.\n",
      "TRUE ENDING:\t I have very fond memories of checkers.\n",
      "FALSE ENDING:\t To this day I hate checkers.\n",
      "\n",
      "\n",
      "**********VALIDATION SET**********\n",
      "\n",
      "Rick grew up in a troubled household. He never found good support in family, and turned to gangs. It wasn't long before Rick got shot in a robbery. The incident caused him to turn a new leaf.\n",
      "TRUE ENDING:\t He is happy now.\n",
      "FALSE ENDING:\t He joined a gang.\n",
      "\n",
      "\n",
      "Laverne needs to prepare something for her friend's party. She decides to bake a batch of brownies. She chooses a recipe and follows it closely. Laverne tests one of the brownies to make sure it is delicious.\n",
      "TRUE ENDING:\t The brownies are so delicious Laverne eats two of them.\n",
      "FALSE ENDING:\t Laverne doesn't go to her friend's party.\n",
      "\n",
      "\n",
      "Sarah had been dreaming of visiting Europe for years. She had finally saved enough for the trip. She landed in Spain and traveled east across the continent. She didn't like how different everything was.\n",
      "TRUE ENDING:\t Sarah decided that she preferred her home over Europe.\n",
      "FALSE ENDING:\t Sarah then decided to move to Europe.\n",
      "\n",
      "\n",
      "Gina was worried the cookie dough in the tube would be gross. She was very happy to find she was wrong. The cookies from the tube were as good as from scratch. Gina intended to only eat 2 cookies and save the rest.\n",
      "TRUE ENDING:\t Gina liked the cookies so much she ate them all in one sitting.\n",
      "FALSE ENDING:\t Gina gave the cookies away at her church.\n",
      "\n",
      "\n",
      "It was  my final performance in marching band. I was playing the snare drum in the band. We played Thriller and Radar Love. The performance was flawless.\n",
      "TRUE ENDING:\t I was very proud of my performance.\n",
      "FALSE ENDING:\t I was very ashamed of my performance.\n",
      "\n",
      "\n",
      "I had been giving this homeless man change everyday. He was on the same corner near my house. One day, as I was driving through my neighborhood I saw a new car. Soon enough, I saw the same homeless man emerge from it!\n",
      "TRUE ENDING:\t I never gave the man money again.\n",
      "FALSE ENDING:\t The next day I gave the man twenty dollars.\n",
      "\n",
      "\n",
      "Jim found an old disposable camera in the bottom of his junk drawer. He began snapping away at everything around him. The counter clicked down to one final photo. The gravity of the situation began to dawn on Jim.\n",
      "TRUE ENDING:\t Jim took time to decide what he would take a picture of.\n",
      "FALSE ENDING:\t Jim took 20 more photos.\n",
      "\n",
      "\n",
      "Ron started his new job as a landscaper today. He loves the outdoors and has always enjoyed working in it. His boss tells him to re-sod the front yard of the mayor's home. Ron is ecstatic, but does a thorough job and finishes super early.\n",
      "TRUE ENDING:\t His boss commends him for a job well done.\n",
      "FALSE ENDING:\t Ron is immediately fired for insubordination.\n",
      "\n",
      "\n",
      "John and Billy became very skilled at beer pong. They entered a contest in college. They won the contest and advanced to the next level. The next level sent them to Vegas.\n",
      "TRUE ENDING:\t In Vegas, John and Billy competed against eighty contestants.\n",
      "FALSE ENDING:\t John and Billy were disappointed.\n",
      "\n",
      "\n",
      "Caroline was a student in medical school. Caroline worked very hard to get good grades. One day Caroline failed a test by one point. Caroline was very frustrated but she continued to study hard.\n",
      "TRUE ENDING:\t Later, she passed the test.\n",
      "FALSE ENDING:\t But she gave up.\n",
      "\n",
      "\n",
      "Trish hated the outdoors. Her friends convinced her to go camping. She wasn't having a good time. They showed her how to fish and showed her the stars.\n",
      "TRUE ENDING:\t Eventually Trish grudgingly came to accept nature.\n",
      "FALSE ENDING:\t The fish had very interesting mating habits.\n",
      "\n",
      "\n",
      "A man walked up to Corey as he pumped gas into his car. The guy admired Corey's car and started a conversation. Corey felt that something was wrong. When he finished pumping gas, the man punched Corey.\n",
      "TRUE ENDING:\t When Corey came to, both the man and his car were gone.\n",
      "FALSE ENDING:\t Corey didn't mind at all.\n",
      "\n",
      "\n",
      "Marcy received a valentine from her boyfriend. It was a card with a gift card in it for chocolates. She was happy and immediately ordered the chocolates. Her boyfriend came over and asked if she liked it.\n",
      "TRUE ENDING:\t Marcy gave her boyfriend a big kiss and said yes.\n",
      "FALSE ENDING:\t Marcy shrugged her shoulders and reached for the TV remote.\n",
      "\n",
      "\n",
      "Cara went to a food cart in Philadelphia the other day. She stopped by an amazing falafel cart. The food was amazing. Cara couldn't believe how good it was.\n",
      "TRUE ENDING:\t Cara decided to eat at the food cart every week.\n",
      "FALSE ENDING:\t Cara told the owner that the food was awful.\n",
      "\n",
      "\n",
      "Tony was happy to be going on his first cruise. He arrived at the docks and was ready with all his luggage. When he walked around the corner he couldn't believe the size of it. He boarded and the ship left the docks.\n",
      "TRUE ENDING:\t Tony enjoyed going on the cruise.\n",
      "FALSE ENDING:\t Tony was scared of the ocean.\n",
      "\n",
      "\n",
      "Ignacio wants to play a sport while he is in college. Since he was a good swimmer, he decides to try out for swim the team. Ignacio makes it onto the team easily. At the first swim meet, Ignacio wins second place!\n",
      "TRUE ENDING:\t Ignacio won a silver medal.\n",
      "FALSE ENDING:\t Ignacio gave up swimming.\n",
      "\n",
      "\n",
      "Danny bought a boat. His nearby marina was having a race. He decided to enter. Danny and his best friend manned the boat.\n",
      "TRUE ENDING:\t They prepared for the start of the race.\n",
      "FALSE ENDING:\t Danny decided to go to sleep.\n",
      "\n",
      "\n",
      "At school, Mary received an assignment to write an essay about pandas. Luckily, Mary loved pandas, as was excited to get started. As soon as she got home, Mary booted up her computer and began. She wrote about all she knew and researched a little too!\n",
      "TRUE ENDING:\t Mary couldn't wait to present her report.\n",
      "FALSE ENDING:\t Mary disliked Pandas.\n",
      "\n",
      "\n",
      "Ellen dreamed of winning a prize for her roses. She planned to enter her special purple rose at the fair. She fertilized the rose bush and covered it each night. The roses grew more beautiful every day.\n",
      "TRUE ENDING:\t Ellen ended up winning the prize.\n",
      "FALSE ENDING:\t Ellen had to pay a replacement fee for her library card.\n",
      "\n",
      "\n",
      "Jesse had just started fifth grade. He tried to make friends with the other boys, but he couldn't. He decided to make friends with the girls instead. They girls welcomed him into their social groups happily.\n",
      "TRUE ENDING:\t Jesse was picked on by the other boys in school.\n",
      "FALSE ENDING:\t Jesse went to the park with his new friend, Roger.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the datasets\n",
    "data_train_raw = pd.read_csv(\"ROCStories_winter2017.csv\")\n",
    "data_test_raw = pd.read_csv(\"cloze_test_2016.csv\")\n",
    "data_val_raw = pd.read_csv(\"cloze_val_2016.csv\")\n",
    "\n",
    "\n",
    "#test set \n",
    "#concat the context sentences together\n",
    "test_context = [\n",
    "    i + ' ' + j + ' ' + k + ' ' + l \n",
    "    for i, j, k, l in \n",
    "                zip(data_test_raw.InputSentence1.tolist() ,data_test_raw.InputSentence2.tolist(),\n",
    "                    data_test_raw.InputSentence3.tolist(),data_test_raw.InputSentence4.tolist())\n",
    "]\n",
    "\n",
    "endings = data_test_raw.RandomFifthSentenceQuiz1.tolist(),data_test_raw.RandomFifthSentenceQuiz2.tolist()\n",
    "answers =data_test_raw.AnswerRightEnding.tolist()\n",
    "#seperate the endings for each example into appropriate lists\n",
    "test_true = []\n",
    "test_false = []\n",
    "num_instances , num_columns = data_test_raw.shape\n",
    "for i in range(num_instances):\n",
    "    test_true.append(endings[answers[i]-1][i])\n",
    "    test_false.append(endings[(answers[i]+2)%2][i])\n",
    "    \n",
    "#train set\n",
    "train_context = [i + ' ' + j +' ' + k + ' ' + l for i, j, k, l in \n",
    "                 zip(data_train_raw.sentence1.tolist(),data_train_raw.sentence2.tolist(),\n",
    "                     data_train_raw.sentence3.tolist(),data_train_raw.sentence4.tolist())]\n",
    "train_end = data_train_raw.sentence5.tolist()\n",
    "\n",
    "# validation set (duplicate code from above, probably couldve just made a function to process the test and\n",
    "# val sets but oh well)\n",
    "val_context = [i + ' ' + j +' ' + k + ' ' + l for i, j, k, l in \n",
    "                zip(data_val_raw.InputSentence1.tolist() ,data_val_raw.InputSentence2.tolist(),\n",
    "                    data_val_raw.InputSentence3.tolist(),data_val_raw.InputSentence4.tolist())]\n",
    "val_endings = data_val_raw.RandomFifthSentenceQuiz1.tolist(),data_val_raw.RandomFifthSentenceQuiz2.tolist()\n",
    "val_answers =data_val_raw.AnswerRightEnding.tolist()\n",
    "#seperate the endings for each example into appropriate lists\n",
    "val_true = []\n",
    "val_false = []\n",
    "num_instances , num_columns = data_val_raw.shape\n",
    "for i in range(num_instances):\n",
    "    val_true.append(val_endings[val_answers[i]-1][i])\n",
    "    val_false.append(val_endings[(val_answers[i]+2)%2][i])\n",
    "    \n",
    "#check the first 20 examples in the test and val set to see how it looks\n",
    "print(\"**********TEST SET**********\\n\")\n",
    "for i in range(20):\n",
    "    print(test_context[i])\n",
    "    print(\"TRUE ENDING:\\t\", test_true[i])\n",
    "    print(\"FALSE ENDING:\\t\", test_false[i])\n",
    "    print(\"\\n\")\n",
    "    \n",
    "print(\"**********VALIDATION SET**********\\n\")\n",
    "for i in range(20):\n",
    "    print(val_context[i])\n",
    "    print(\"TRUE ENDING:\\t\", val_true[i])\n",
    "    print(\"FALSE ENDING:\\t\", val_false[i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further playing can be done with the dataset, but we now have the base components needed to use for a simple model to get a proper evaluation framework set up. \n",
    "\n",
    "The train set only has the proper ending for stories, unlike the test and validation set, where there are 2 possible endings. If we want a train corpus that includes false stories endings as well as the true endings, we could assign random endings to half of the dataset. Each instance in the dataset should then have a label marking it as a coherent or incoherent story.\n",
    "\n",
    "As a first approach, we could modify the dataset to turn this into a binary classification problem. We have a functional text classifier from HW4, it should be straightforward to implement with our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vocabulary class courtesy of HW4\n",
    "\n",
    "class MyVocabulary:\n",
    "    def __init__(self, special_tokens=None):\n",
    "        self.w2idx = {}\n",
    "        self.idx2w = {}\n",
    "        self.w2cnt = defaultdict(int)\n",
    "        self.special_tokens = special_tokens\n",
    "        if self.special_tokens is not None:\n",
    "            self.add_tokens(special_tokens)\n",
    "\n",
    "    def add_tokens(self, tokens):\n",
    "        for token in tokens:\n",
    "            self.add_token(token)\n",
    "            self.w2cnt[token] += 1\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token not in self.w2idx:\n",
    "            cur_len = len(self)\n",
    "            self.w2idx[token] = cur_len\n",
    "            self.idx2w[cur_len] = token\n",
    "\n",
    "    def prune(self, min_cnt=2):\n",
    "        to_remove = set([token for token in self.w2idx if self.w2cnt[token] < min_cnt])\n",
    "        #print(set(self.special_tokens))\n",
    "        \n",
    "        #INSERTED THESE CHANGES MYSELF\n",
    "        for token in self.special_tokens:\n",
    "            if token in to_remove:\n",
    "                to_remove.remove(token)\n",
    "        #THE ORIGINAL CODE IN PRUNE WAS REMOVING <PAD> FROM THE VOCAB\n",
    "        #to_remove ^= set(self.special_tokens)\n",
    "        #print('<PAD>' in to_remove,self.w2cnt['<PAD>'])\n",
    "\n",
    "        for token in to_remove:\n",
    "            self.w2cnt.pop(token)\n",
    "\n",
    "        self.w2idx = {token: idx for idx, token in enumerate(self.w2cnt.keys())}\n",
    "        self.idx2w = {idx: token for token, idx in self.w2idx.items()}\n",
    "        \n",
    "        #print(\"Pad in w2cnt\", '<PAD>' in self.w2cnt)\n",
    "        #print(\"Pad in w2idx\", '<PAD>' in self.w2idx)\n",
    "        #print(\"Pad in idx2w\", self.idx2w[0] is '<PAD>')\n",
    "        #print(self.w2idx['<PAD>'])\n",
    "\n",
    "    def __contains__(self, item):\n",
    "        return item in self.w2idx\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if isinstance(item, str):\n",
    "            return self.w2idx[item]\n",
    "        elif isinstance(item , int):\n",
    "            return self.idx2w[item]\n",
    "        else:\n",
    "            raise TypeError(\"Supported indices are int and str\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return(len(self.w2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text classification dataset from HW4\n",
    "\n",
    "class TextClassificationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels, vocab=None, labels_vocab=None, max_len=50, lowercase=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texts (list of str): texts of the dataset examples\n",
    "            labels (list of str): the correponding labels of the dataset examples\n",
    "            vocab (MyVocabulary, optional): vocabular to convert text to indices. If not provided, will be created based on the texts\n",
    "            labels_vocab (MyVocabulary, optional): vocabular to convert labels to indices. If not provided, will be created based on the labels\n",
    "            max_len (int): maximum length of the text. Texts shorter than max_len will be cut at the end\n",
    "            lowercase (bool, optional): a fag specifying whether or not the input text should be lowercased\n",
    "        \"\"\"\n",
    "        \n",
    "        self.max_len = max_len\n",
    "        self.lowercase = lowercase\n",
    "        \n",
    "\n",
    "        self.texts = [self._preprocess(t, max_len=max_len, lowercase=lowercase) for t in texts]\n",
    "        self.labels = labels\n",
    "\n",
    "        if vocab is None:\n",
    "            vocab = MyVocabulary(['<PAD>', '<UNK>'])\n",
    "            for text in self.texts:\n",
    "                vocab.add_tokens(text)\n",
    "            \n",
    "        if labels_vocab is None:\n",
    "            labels_vocab = MyVocabulary()\n",
    "            labels_vocab.add_tokens(labels)\n",
    "            \n",
    "        self.vocab = vocab\n",
    "        self.labels_vocab = labels_vocab\n",
    "        \n",
    "    def _preprocess(self, text, max_len=None, lowercase=True):\n",
    "        \"\"\"\n",
    "        Preprocess a give dataset example\n",
    "        Args:\n",
    "            text (str): given dataset example\n",
    "            max_len (int, optional): maximum sequence length\n",
    "            lowercase (bool, optional): a fag specifying whether or not the input text should be lowercased\n",
    "        \n",
    "        Returns:\n",
    "            a list of tokens for a given text span\n",
    "        \"\"\" \n",
    "        ### YOUR CODE BELOW ###\n",
    "        if lowercase:\n",
    "            text = text.lower()\n",
    "        # tokenize the input text\n",
    "       \n",
    "        tokens = []\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # cut the list of tokens to `max_len` if needed \n",
    "        if len(tokens) > max_len:\n",
    "            del tokens[max_len:]\n",
    "        tokens = self._pad(tokens)\n",
    "        ### YOUR CODE ABOVE ###\n",
    "        return tokens\n",
    "    \n",
    "    def _pad(self, tokens):\n",
    "        \"\"\"\n",
    "        Pad tokens to self.max_len\n",
    "        Args:\n",
    "            tokens (list): a list of str tokens for a given example\n",
    "            \n",
    "        Returns:\n",
    "            list: a padded list of str tokens for a given example\n",
    "        \"\"\"\n",
    "        # pad the list of tokens to be exactly of the `max_len` size\n",
    "        ### YOUR CODE BELOW ###\n",
    "        if len(tokens) < self.max_len:\n",
    "            for i in range(len(tokens), self.max_len):\n",
    "                tokens.append(\"<PAD>\") \n",
    "        ### YOUR CODE ABOVE ###\n",
    "        return tokens\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Given an index, return a formatted dataset example\n",
    "        \n",
    "        Args:\n",
    "            idx (int): dataset index\n",
    "            \n",
    "        Returns:\n",
    "            tuple: a tuple of token_ids based on the vocabulary mapping  and a corresponding label\n",
    "        \"\"\"\n",
    "        ### YOUR CODE BELOW ###\n",
    "        tokens = []\n",
    "        words = self.texts[idx]\n",
    "        for word in words:\n",
    "            if word not in self.vocab.w2cnt:\n",
    "                #print(word)\n",
    "                tokens.append(self.vocab.w2idx['<UNK>'])\n",
    "            elif word != '<PAD>':\n",
    "                tokens.append(self.vocab.w2idx[word])\n",
    "            elif word == '<PAD>':\n",
    "                tokens.append(self.vocab.w2idx[\"<PAD>\"])\n",
    "                #print(\"found pad\")\n",
    "        label = self.labels[idx]\n",
    "        ### YOUR CODE ABOVE ###\n",
    "        \n",
    "        return  np.asarray(tokens), np.asarray(self.labels_vocab.w2idx[label])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets modify the datasets.\n",
    "A train set should have the full five sentence story, as well as a label marking it as a coherent story (with the correct expected ending) or an incoherent story (false ending). \n",
    "\n",
    "The challenge from this approach is the structure of the original train set, as discussed above. We propose that taking half of the train examples, and shuffling the endings around randomly should suffice, but it also seems likely that this may not give the kind of dataset that would be ideal. Stories marked as incoherent will presumably have endings that are completely unrelated to the origianl story. This will contrast with the premade false endings in our test dataset, which will seem much more likely as possible coherent endings in comparison to the modified train set stories. Our prediction is that the accuracy will not be great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 6, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "#code taken from \n",
    "#https://stackoverflow.com/questions/9557182/python-shuffle-only-some-elements-of-a-list\n",
    "class MutableSlice(object):\n",
    "    def __init__(self, baselist, begin, end=None):\n",
    "        self._base = baselist\n",
    "        self._begin = begin\n",
    "        self._end = len(baselist) if end is None else end\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._end - self._begin\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self._base[self._begin + i]\n",
    "\n",
    "    def __setitem__(self, i, val):\n",
    "        self._base[i + self._begin] = val\n",
    "        \n",
    "mylist = [1,2,3,4,5,6]\n",
    "slice = MutableSlice(mylist, 2)\n",
    "import random\n",
    "random.shuffle(slice)\n",
    "print(mylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joy wanted to find the perfect shell for her hermit crab. She could buy one, but it would be more special if she found it. She scoured the beach all morning. Everything that might work was broken! She had to buy a shell after all. \n",
      "coherent \n",
      "\n",
      "Steph usually ate a large breakfast. But her new job meant she had less time to make food. So Steph decided to eat simpler food and smaller portions. And in a month, she lost 10 pounds. Her doctor said she was healthier, too. \n",
      "coherent \n",
      "\n",
      "Jen brought her cat Lee to the vet for a checkup. The veterinarian, Doctor Mike, found three fleas on Lee. Doctor Mike suggested a flea dip to Jen. Jen refused because she did not want Lee to be dipped in chemicals. The vet suggested a flea collar for Jen's pet German shepherd. \n",
      "incoherent \n",
      "\n",
      "Sam was stargazing one night. Suddenly he sat bolt upright. He saw a twinkling, moving star! He ran to show it to his dad. His dad laughed because the star was really a jet with lights. \n",
      "coherent \n",
      "\n",
      "Adam needed a new baseball cap. He looked online at different hats. He selected the perfect hat and ordered it. He was so excited to receive it. He threw the box in the closet. \n",
      "incoherent \n",
      "\n",
      "Fred decides that he would like to start remembering his daily life. He goes to the local store and buys a diary. From that day forward, Fred records everything he does in his diary. Year's later Fred references the diary to see how he spent his time. Fred didn't like his diary. \n",
      "incoherent \n",
      "\n",
      "Carmen wanted to make a healthier version of pizza. Carmen had to find a recipe for the pizza. Carmen found a recipe that had a cauliflower crust. Carmen went to the store to purchase the ingredients. Carmen's family loved the crust. \n",
      "coherent \n",
      "\n",
      "Ivan got bitten by a mosquito on a forest hike. The bite was red and itchy, but it didn't bother him at first. Then it swelled up terribly and became sore. A doctor looked at it and told Ivan the bite was infected! Ivan was happy the hike ended so well. \n",
      "incoherent \n",
      "\n",
      "Jay was finishing up lunch at McDonald's, throwing away his trash. As soon as he threw it away, he realized there was a promo happening. There were some game pieces on his fry box so he fished it out. He peeled off the game piece and couldn't believe it. He'd dug in the trash for nothing. \n",
      "coherent \n",
      "\n",
      "Terry loves to swim but he never learned how to dive. He always jumps in the water feet first. This summer Terry decides he wants to learn how to dive in the water. He practices diving very hard every day. Terry has given up swimming. \n",
      "incoherent \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#shuffle half of the train endings before appending them to the contexts\n",
    "length = len(train_end)\n",
    "start = int(length/2)\n",
    "stop = length - start\n",
    "end_slice = MutableSlice(train_end, start)\n",
    "\n",
    "#the following for loops surrounding the shuffle are for veryfing proper shuffling of story ends\n",
    "#for i in range(length - 21, length-1):\n",
    "#    print(train_end[i])\n",
    "\n",
    "random.shuffle(end_slice)\n",
    "\n",
    "#print(\"\\n***************************\\n\")\n",
    "\n",
    "#for i in range(length - 21, length-1):\n",
    "#    print(train_end[i])\n",
    "\n",
    "labels_train = []\n",
    "for i in range(start):\n",
    "    labels_train.append('coherent')\n",
    "for i in range(stop):\n",
    "    labels_train.append('incoherent')\n",
    "\n",
    "#append the endings to the context\n",
    "data_train = [i + ' ' + j for i, j in zip(train_context, train_end)]\n",
    "\n",
    "#will do a similair approach to test data. No need for random shuffling of endings to produce incoherent endings\n",
    "labels_test = []\n",
    "mid = int(len(test_context)/2)\n",
    "for i in range(mid):\n",
    "    labels_test.append('coherent')\n",
    "for i in range(len(test_context) - mid):\n",
    "    labels_test.append('incoherent')\n",
    "\n",
    "labels_val = labels_test\n",
    "data_val = [i + ' ' + j for i, j in zip(val_context, val_true[0:mid] + val_false[mid:])]\n",
    "data_test = [i + ' ' + j for i, j in zip(test_context, test_true[0:mid] + test_false[mid:])]\n",
    "\n",
    "#we now have all three sets of labeled data, lets shuffle the sets before training\n",
    "\n",
    "val = list(zip(data_val, labels_val))\n",
    "test = list(zip(data_test, labels_test))\n",
    "train = list(zip(data_train, labels_train))\n",
    "\n",
    "random.shuffle(train)\n",
    "random.shuffle(test)\n",
    "random.shuffle(val)\n",
    "\n",
    "data_val, labels_val = zip(*val)\n",
    "data_train, labels_train = zip(*train)\n",
    "data_test, labels_test = zip(*test)\n",
    "\n",
    "#sanity check, looking at some of the data\n",
    "for i in range(10):\n",
    "    print(data_val[i], '\\n' + labels_val[i], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52665\n",
      "1871\n",
      "1871\n"
     ]
    }
   ],
   "source": [
    "print(len(labels_train))\n",
    "print(len(labels_test))\n",
    "print(len(labels_val))\n",
    "\n",
    "#dataset_train = TextClassificationDataset(data_train, labels_train)\n",
    "dataset_train = TextClassificationDataset(data_val, labels_val)\n",
    "#dataset_val = TextClassificationDataset(data_val, labels_val, vocab = dataset_train.vocab, labels_vocab = dataset_train.labels_vocab )\n",
    "dataset_test = TextClassificationDataset(data_test, labels_test, vocab = dataset_train.vocab, labels_vocab = dataset_train.labels_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6706 words before pruning\n",
      "3815 words after pruning\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_train.vocab.w2cnt), \"words before pruning\")\n",
    "dataset_train.vocab.prune()\n",
    "print(len(dataset_train.vocab.w2cnt),\"words after pruning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize our embedding matrix using glove vectors corresponding to our training vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Glove embeddings\n",
    "#initialize model embedding matrix with glove embeddings\n",
    "embedding_dim = len(dataset_train.vocab)\n",
    "glove_matrix = torch.nn.Embedding(embedding_dim, 300)\n",
    "\n",
    "with open(\"glove.6B.300d.txt\", \"r\") as glove:\n",
    "    for line in glove:\n",
    "        if len(line.split()) > 301:\n",
    "            continue#skip words with spaces\n",
    "        if line.split()[0] in dataset_train.vocab.w2idx:\n",
    "            weight = np.array([float(t) for t in line.strip().split()[1:]])\n",
    "            glove_matrix.weight.data[dataset_train.vocab.w2idx[line.split()[0]]] = torch.FloatTensor(weight)\n",
    "#glove_matrix.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModel(torch.nn.Module):\n",
    "    def __init__(self, embedding_size, vocab_size, hidden_size, nb_classes, pretrained_embeddings):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.nb_classes = nb_classes\n",
    "\n",
    "        ### YOUR CODE BELOW ###\n",
    "        self.embeddings = pretrained_embeddings\n",
    "        \n",
    "        self.LSTM = torch.nn.LSTM(input_size=embedding_size, hidden_size=self.hidden_size, bidirectional = True)\n",
    "        self.projection = torch.nn.Linear(hidden_size * 2, nb_classes)\n",
    "        \n",
    "        #self.LSTM = torch.nn.LSTM(input_size=embedding_size, hidden_size=self.hidden_size, bidirectional = False)\n",
    "        #self.projection = torch.nn.Linear(hidden_size, nb_classes)\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        ### YOUR CODE ABOVE ###\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        ### YOUR CODE BELOW ###\n",
    "        embeds = self.embeddings(inputs)\n",
    "        embeds = self.dropout(embeds)\n",
    "        outputs, hidden = self.LSTM(embeds)\n",
    "        #print(outputs.shape)\n",
    "        #outputs = self.dropout(outputs)\n",
    "        outputs = torch.max(outputs, dim = 1)[0]\n",
    "        logits = self.projection(outputs)\n",
    "        ### YOUR CODE ABOVE ###        \n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATALOADER #\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, shuffle=True, batch_size=64)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, shuffle=False, batch_size=64)\n",
    "#dataloader_val = torch.utils.data.DataLoader(dataset_val, shuffle=False, batch_size=64)\n",
    "\n",
    "# MODEL INITIALIZATION #\n",
    "model = TextClassificationModel(300, len(dataset_train.vocab), 128, 2, glove_matrix)#2 labels\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    \n",
    "# OPTIMIZER #\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# LOSS-FUNCTION #\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss 0.6995687305927276\n",
      "Epoch 1, loss 0.7070395708084106\n",
      "Epoch 2, loss 0.6951139450073243\n",
      "Epoch 3, loss 0.6940716107686361\n",
      "Epoch 4, loss 0.687838633855184\n",
      "Epoch 5, loss 0.6869573950767517\n",
      "Epoch 6, loss 0.6804439624150594\n",
      "Epoch 7, loss 0.6775656561056773\n",
      "Epoch 8, loss 0.6587942997614543\n",
      "Epoch 9, loss 0.6342438101768494\n"
     ]
    }
   ],
   "source": [
    "# TRAINING #\n",
    "losses = [] \n",
    "losses_val = []\n",
    "for epoch in range(10):\n",
    "    epoch_losses = []\n",
    "    epoch_validation_losses = []\n",
    "   \n",
    "    #training loop\n",
    "    for i, batch in enumerate(dataloader_train):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x, y = batch\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "        \n",
    "        #print(y)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        \n",
    "        epoch_losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    \"\"\" #validation loop  \n",
    "    for i, batch in enumerate(dataloader_val):\n",
    "        \n",
    "        x, y = batch\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "        \n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        \n",
    "        epoch_validation_losses.append(loss.item())\"\"\" \n",
    "   \n",
    "        \n",
    "\n",
    "    epoch_loss = np.mean(epoch_losses)\n",
    "    #epoch_loss_val = np.mean(epoch_validation_losses)\n",
    "    losses.append(epoch_loss)\n",
    "    #losses_val.append(epoch_loss_val)\n",
    "    #print('Epoch {}, loss {}, validation loss {}'.format(epoch, epoch_loss, epoch_loss_val))\n",
    "    print('Epoch {}, loss {}'.format(epoch, epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataloader):\n",
    "    \"\"\"\n",
    "    Predict probability distributions over classes on the test data\n",
    "    \n",
    "    Args:\n",
    "        model: your torch.nn.Module() model object\n",
    "        dataloader: test Dataloder() object\n",
    "        \n",
    "    Returns:\n",
    "        tuple: np.array/list with true labels and np.array/list with predicted labels\n",
    "    \"\"\"\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    \n",
    "    ### YOUR CODE BELOW ###\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        x, y = batch\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "            \n",
    "        outputs = model(x).detach().cpu()\n",
    "        y_pred.append(torch.argmax(outputs.data, 1))\n",
    "        y_true.append(y) \n",
    "    ### YOUR CODE ABOVE ###\n",
    "    y_pred = np.concatenate(y_pred, axis=0)\n",
    "    y_true = np.concatenate(y_true, axis=0)    \n",
    "    \n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_train, y_pred_train = predict(model, dataloader_train)\n",
    "y_true_test, y_pred_test = predict(model, dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train 0.7108498129342598\n",
      "Accuracy test 0.5344735435595938\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE BELOW ###\n",
    "acc_train = accuracy_score(y_true_train, y_pred_train)\n",
    "acc_test = accuracy_score(y_true_test, y_pred_test)\n",
    "### YOUR CODE ABOVE ###\n",
    "\n",
    "print('Accuracy train', acc_train)\n",
    "print('Accuracy test', acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
